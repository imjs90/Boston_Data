{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import datetime \n",
    "from datetime import timedelta\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Desktop/Boston.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import set_printoptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = df.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "print(np.shape(X))\n",
    "print(np.shape(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, val, and test sets. In addition we will\n",
    "# create a small development set as a subset of the training data;\n",
    "# we can use this for development so our code runs faster.\n",
    "num_training = 300\n",
    "num_validation = 100\n",
    "num_test = 100\n",
    "num_dev = 6\n",
    "\n",
    "# Our validation set will be num_validation points from the original\n",
    "# training set.\n",
    "mask = range(num_training, num_training + num_validation)\n",
    "X_val = X[mask]\n",
    "y_val = Y[mask]\n",
    "\n",
    "# Our training set will be the first num_train points from the original\n",
    "# training set.\n",
    "mask = range(num_training)\n",
    "X_train = X[mask]\n",
    "y_train = Y[mask]\n",
    "\n",
    "# We will also make a development set, which is a small subset of\n",
    "# the training set.\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X[mask]\n",
    "y_dev = Y[mask]\n",
    "\n",
    "# We use the first num_test points of the original test set as our\n",
    "# test set.\n",
    "mask = range(num_test)\n",
    "X_test = X[mask]\n",
    "y_test = Y[mask]\n",
    "\n",
    "print('Train data shape:        ', X_train.shape)\n",
    "print('Train labels shape:      ', y_train.shape)\n",
    "print('Validation data shape:   ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape:         ', X_test.shape)\n",
    "print('Test labels shape:       ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaler = StandardScaler()\n",
    "X_scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "print('mean=',np.mean(X_train_scaled, axis = 0), '\\nstd=', np.std(X_train_scaled, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_scaled = X_scaler.transform(X_val)\n",
    "print('mean=',np.mean(X_val_scaled, axis = 0), '\\nstd=', np.std(X_val_scaled, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "print('mean=',np.mean(X_test_scaled, axis = 0), '\\nstd=', np.std(X_test_scaled, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape([300,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LinearRegressionModel import affine_forward, MSE_loss, affine_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = W = 0.0001 * np.random.randn(13,1)\n",
    "b = np.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = affine_forward(X_train_scaled, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = MSE_loss(Y_pred, y_train, W, reg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW,db = affine_backward(X_train_scaled, y_train, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = len(y_train)\n",
    "num_iterations = 2500\n",
    "learning_rate = 0.001\n",
    "reg = 0\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for i in range(num_iterations + 1):\n",
    "    \n",
    "    # forward step\n",
    "    Y_pred = affine_forward(X_train_scaled, W, b)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = MSE_loss(Y_pred, y_train, W, reg=0)       \n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    # backward step\n",
    "    dW,db = affine_backward(X_train_scaled, y_train, Y_pred)\n",
    "      \n",
    "    # report every 100 iteration\n",
    "    if i % 100 == 0:\n",
    "        y_preds = np.argmax(Y_pred, axis=1)\n",
    "        train_accuracy = 100.0 * np.mean(y_preds ==  y_train)\n",
    "        \n",
    "        val_pred = affine_forward(X_val, W, b)\n",
    "        val_preds = np.argmax(val_pred, axis=1)\n",
    "        val_accuracy = 100.0 * np.mean(val_preds == y_val)\n",
    "        print(\"Iteration %4d: loss = %5.2f | train accuracy = %5.2f | validation accuracy = %5.2f\" % \n",
    "             (i, loss, train_accuracy, val_accuracy))\n",
    "    \n",
    "    # update rule\n",
    "    W = W - learning_rate * dW\n",
    "    b = b - learning_rate * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = len(y_train)\n",
    "num_iterations = 2500\n",
    "learning_rate = 0.001\n",
    "reg = 0\n",
    "\n",
    "#Initializing parameters\n",
    "W = W = 0.0001 * np.random.randn(13,1)\n",
    "b = np.zeros(1)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "#learning rate value set to 0.01\n",
    "\n",
    "for i in range(num_iterations + 1):\n",
    "    #Calculate loss\n",
    "    Y_pred = np.dot(X_train_scaled,W) + b \n",
    "    Loss_error = 0.5 * (Y_pred - y_train)**2\n",
    "    loss = np.sum(Loss_error)/M\n",
    "    loss += 0.5 * reg * np.sum(W * W) \n",
    "    #Calculate dW and db    \n",
    "    dW = -(2/M) * sum(X_train_scaled * (Y_pred - y_train))\n",
    "    db = -(2/M) * sum(Y_pred - y_train)\n",
    "    loss_history.append(loss)\n",
    "   \n",
    "  #Update parameters:\n",
    "    \n",
    "    W = W - learning_rate * dW\n",
    "    b = b - learning_rate * db\n",
    "     \n",
    "    if i%10 == 0:\n",
    "        print(\"Cost at\", i,\"iteration = \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
